from app.config.config import settings
import os
from app.services.rate_limit import check_rate_limit


def transcribe(audio_path: str) -> dict:
    client = settings.openai_client

    if settings.is_ai_ready:
        check_rate_limit("whisper")

    # MOCK LOGIC: Always return mock if AI is disabled or key is missing
    if not settings.AI_ENABLED or not client:
        print(f"ğŸ› ï¸  MOCK WHISPER: Simulating transcription for {audio_path}")
        return {
            "text": "ã“ã‚“ã«ã¡ã¯ã€å…ƒæ°—ã§ã™ã‹ï¼Ÿ ã¯ã„ã€å…ƒæ°—ã§ã™ï¼",
            "segments": [
                {
                    "id": 0,
                    "start": 0.0,
                    "end": 2.5,
                    "text": "ã“ã‚“ã«ã¡ã¯ã€å…ƒæ°—ã§ã™ã‹ï¼Ÿ",
                },
                {"id": 1, "start": 2.6, "end": 4.5, "text": "ã¯ã„ã€å…ƒæ°—ã§ã™ï¼"},
            ],
            "duration": 4.5,
        }

    if not os.path.exists(audio_path):
        raise FileNotFoundError(f"Audio file not found: {audio_path}")

    try:
        with open(audio_path, "rb") as audio:
            response = client.audio.transcriptions.create(
                model="whisper-1", file=audio, response_format="verbose_json"
            )
            # Response is a Transcription object, convert to dict for downstream services
            transcript = response.model_dump()
        print("ğŸ™ï¸ Whisper transcription successful.")
    except Exception as e:
        raise RuntimeError(f"âš ï¸ Failed to transcribe audio: {str(e)}") from e

    return transcript
